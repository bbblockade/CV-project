{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4922,"status":"ok","timestamp":1697533849858,"user":{"displayName":"Chen Jiqiang","userId":"04210601668986502227"},"user_tz":-660},"id":"BZvLXSqmounG"},"outputs":[],"source":["import torch.nn as nn\n","import torchvision.models as models\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import os\n","from PIL import Image\n","from torchvision.models import resnet50, ResNet50_Weights, ResNet18_Weights, resnet18, EfficientNet_B0_Weights, efficientnet_b0\n","import random\n","import torch\n","class SiameseResNet(nn.Module):\n","    def __init__(self):\n","        super(SiameseResNet, self).__init__()\n","\n","\n","        self.resnet18  = models.resnet18()\n","        ct = 0\n","        for child in self.resnet18.children():\n","            ct += 1\n","            if ct < 9:\n","                for param in child.parameters():\n","                    param.requires_grad = False\n","\n","        blocks = list(self.resnet18.layer4.children())[-1]\n","\n","        for param in blocks.parameters():\n","                param.requires_grad = True\n","        # Remove the final layer to get embeddings\n","        self.features = nn.Sequential(*list(self.resnet18.children())[:-1])\n","\n","        self.dense = nn.Sequential(\n","            nn.Linear(512, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","        )\n","\n","\n","\n","    def forward_one(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size()[0], -1)\n","        x = self.dense(x)\n","        return x\n","\n","    def forward(self, anchor, positive, negative):\n","        anchor_embedding = self.forward_one(anchor)\n","        positive_embedding = self.forward_one(positive)\n","        negative_embedding = self.forward_one(negative)\n","        return anchor_embedding, positive_embedding, negative_embedding"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1252,"status":"ok","timestamp":1697533902059,"user":{"displayName":"Chen Jiqiang","userId":"04210601668986502227"},"user_tz":-660},"id":"yAIXSeyp0SbH","outputId":"d041788f-c921-4f21-cbd0-2f4de3cfd92d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Module resnet18.conv1.weight is frozen\n","Module resnet18.bn1.weight is frozen\n","Module resnet18.bn1.bias is frozen\n","Module resnet18.layer1.0.conv1.weight is frozen\n","Module resnet18.layer1.0.bn1.weight is frozen\n","Module resnet18.layer1.0.bn1.bias is frozen\n","Module resnet18.layer1.0.conv2.weight is frozen\n","Module resnet18.layer1.0.bn2.weight is frozen\n","Module resnet18.layer1.0.bn2.bias is frozen\n","Module resnet18.layer1.1.conv1.weight is frozen\n","Module resnet18.layer1.1.bn1.weight is frozen\n","Module resnet18.layer1.1.bn1.bias is frozen\n","Module resnet18.layer1.1.conv2.weight is frozen\n","Module resnet18.layer1.1.bn2.weight is frozen\n","Module resnet18.layer1.1.bn2.bias is frozen\n","Module resnet18.layer2.0.conv1.weight is frozen\n","Module resnet18.layer2.0.bn1.weight is frozen\n","Module resnet18.layer2.0.bn1.bias is frozen\n","Module resnet18.layer2.0.conv2.weight is frozen\n","Module resnet18.layer2.0.bn2.weight is frozen\n","Module resnet18.layer2.0.bn2.bias is frozen\n","Module resnet18.layer2.0.downsample.0.weight is frozen\n","Module resnet18.layer2.0.downsample.1.weight is frozen\n","Module resnet18.layer2.0.downsample.1.bias is frozen\n","Module resnet18.layer2.1.conv1.weight is frozen\n","Module resnet18.layer2.1.bn1.weight is frozen\n","Module resnet18.layer2.1.bn1.bias is frozen\n","Module resnet18.layer2.1.conv2.weight is frozen\n","Module resnet18.layer2.1.bn2.weight is frozen\n","Module resnet18.layer2.1.bn2.bias is frozen\n","Module resnet18.layer3.0.conv1.weight is frozen\n","Module resnet18.layer3.0.bn1.weight is frozen\n","Module resnet18.layer3.0.bn1.bias is frozen\n","Module resnet18.layer3.0.conv2.weight is frozen\n","Module resnet18.layer3.0.bn2.weight is frozen\n","Module resnet18.layer3.0.bn2.bias is frozen\n","Module resnet18.layer3.0.downsample.0.weight is frozen\n","Module resnet18.layer3.0.downsample.1.weight is frozen\n","Module resnet18.layer3.0.downsample.1.bias is frozen\n","Module resnet18.layer3.1.conv1.weight is frozen\n","Module resnet18.layer3.1.bn1.weight is frozen\n","Module resnet18.layer3.1.bn1.bias is frozen\n","Module resnet18.layer3.1.conv2.weight is frozen\n","Module resnet18.layer3.1.bn2.weight is frozen\n","Module resnet18.layer3.1.bn2.bias is frozen\n","Module resnet18.layer4.0.conv1.weight is frozen\n","Module resnet18.layer4.0.bn1.weight is frozen\n","Module resnet18.layer4.0.bn1.bias is frozen\n","Module resnet18.layer4.0.conv2.weight is frozen\n","Module resnet18.layer4.0.bn2.weight is frozen\n","Module resnet18.layer4.0.bn2.bias is frozen\n","Module resnet18.layer4.0.downsample.0.weight is frozen\n","Module resnet18.layer4.0.downsample.1.weight is frozen\n","Module resnet18.layer4.0.downsample.1.bias is frozen\n","Module resnet18.layer4.1.conv1.weight is NOT frozen\n","Module resnet18.layer4.1.bn1.weight is NOT frozen\n","Module resnet18.layer4.1.bn1.bias is NOT frozen\n","Module resnet18.layer4.1.conv2.weight is NOT frozen\n","Module resnet18.layer4.1.bn2.weight is NOT frozen\n","Module resnet18.layer4.1.bn2.bias is NOT frozen\n","Module resnet18.fc.weight is NOT frozen\n","Module resnet18.fc.bias is NOT frozen\n","Module dense.0.weight is NOT frozen\n","Module dense.0.bias is NOT frozen\n","Module dense.1.weight is NOT frozen\n","Module dense.1.bias is NOT frozen\n","Module dense.3.weight is NOT frozen\n","Module dense.3.bias is NOT frozen\n"]}],"source":["def check_frozen(model):\n","    for name, param in model.named_parameters():\n","        if not param.requires_grad:\n","            print(f\"Module {name} is frozen\")\n","        else:\n","            print(f\"Module {name} is NOT frozen\")\n","\n","# example usage:\n","model = SiameseResNet()\n","check_frozen(model)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2256,"status":"ok","timestamp":1697533935458,"user":{"displayName":"Chen Jiqiang","userId":"04210601668986502227"},"user_tz":-660},"id":"bSHLRUpaounP"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","pair_train = pd.read_csv('pair.csv')\n","anchors = []\n","positives = []\n","negatives = []\n","for index, row in pair_train.iterrows():\n","    for negative in row[2:]:\n","        anchors.append(row[0])\n","        positives.append(row[1])\n","        negatives.append(negative)\n","\n","\n","# Combine the lists into a single list of tuples\n","data = list(zip(anchors, positives, negatives))\n","\n","# Split the combined list into training and validation sets\n","train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","# Separate the training and validation sets into anchors, positives, and negatives\n","train_anchors, train_positives, train_negatives = zip(*train_data)\n","val_anchors, val_positives, val_negatives = zip(*val_data)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1697533935458,"user":{"displayName":"Chen Jiqiang","userId":"04210601668986502227"},"user_tz":-660},"id":"1A29y9UEounQ"},"outputs":[],"source":["\n","class ImageTripletDataset(Dataset):\n","    def __init__(self, anchors, positives, negatives, tensor_dict_left, tensor_dict_right, all_keys_right):\n","        self.anchors = anchors\n","        self.positives = positives\n","        self.negatives = negatives\n","        self.tensor_dict_left = tensor_dict_left\n","        self.tensor_dict_right = tensor_dict_right\n","        self.all_keys_right = all_keys_right\n","\n","\n","    def __getitem__(self, idx):\n","        anchor = self.tensor_dict_left[self.anchors[idx]]\n","        positive = self.tensor_dict_right[self.positives[idx]]\n","\n","        if random.random() < 0.5:\n","            possible_keys = self.all_keys_right.copy()\n","            possible_keys.remove(self.positives[idx])\n","            possible_keys.remove(self.negatives[idx])\n","\n","            negative = self.tensor_dict_right[random.choice(all_keys_right)]\n","\n","        else:\n","            negative = self.tensor_dict_right[self.negatives[idx]]\n","\n","        return anchor, positive, negative\n","\n","    def __len__(self):\n","        return len(self.anchors)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2568,"status":"ok","timestamp":1697533938024,"user":{"displayName":"Chen Jiqiang","userId":"04210601668986502227"},"user_tz":-660},"id":"LnX_sgzvqduI"},"outputs":[],"source":["def load_tensors_from_folder(tensor_folder):\n","    tensor_files = [f for f in os.listdir(tensor_folder) if f.endswith('.pt')]\n","    tensor_dict = {}\n","\n","    for tensor_file in tensor_files:\n","        tensor_path = os.path.join(tensor_folder, tensor_file)\n","        tensor_name = os.path.splitext(tensor_file)[0]  # Remove the .pt extension\n","        tensor_dict[tensor_name] = torch.load(tensor_path)\n","\n","    return tensor_dict\n","\n","# Usage example\n","tensor_folder_left = '/content/tensor/train_tensor/left'\n","tensor_dict_left = load_tensors_from_folder(tensor_folder_left)\n","tensor_folder_right = '/content/tensor/train_tensor/right'\n","tensor_dict_right = load_tensors_from_folder(tensor_folder_right)\n","all_keys_right =  list(tensor_dict_right.keys())"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1697535236942,"user":{"displayName":"Chen Jiqiang","userId":"04210601668986502227"},"user_tz":-660},"id":"5S37R7bMvDnz"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","import torch\n","device = torch.device('cuda')\n","model = SiameseResNet().to(device)\n","criterion = nn.TripletMarginLoss(margin=1, p=2, eps=1e-7)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","train_data_loader = DataLoader(ImageTripletDataset(train_anchors, train_positives, train_negatives, tensor_dict_left, tensor_dict_right, all_keys_right), batch_size=64, shuffle=True)\n","val_data_loader = DataLoader(ImageTripletDataset(val_anchors, val_positives, val_negatives, tensor_dict_left, tensor_dict_right, all_keys_right), batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":722319,"status":"ok","timestamp":1697535973105,"user":{"displayName":"Chen Jiqiang","userId":"04210601668986502227"},"user_tz":-660},"id":"f4gcDfXIounQ","outputId":"19a95691-8a78-4183-aef8-67b2180fe5ab"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/15: 100%|██████████| 475/475 [01:20<00:00,  5.87it/s, loss=0.442]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/15, Average Loss: 0.4415504180130206\n","Epoch 1/15, Validation Loss: 0.12569783847121632\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/15: 100%|██████████| 475/475 [01:24<00:00,  5.61it/s, loss=0.0759]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/15, Average Loss: 0.07593917252201783\n","Epoch 2/15, Validation Loss: 0.04191102348894131\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/15: 100%|██████████| 475/475 [01:24<00:00,  5.64it/s, loss=0.0356]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/15, Average Loss: 0.03561429577438455\n","Epoch 3/15, Validation Loss: 0.022518869043335693\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/15: 100%|██████████| 475/475 [01:25<00:00,  5.57it/s, loss=0.0251]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/15, Average Loss: 0.025070244795397708\n","Epoch 4/15, Validation Loss: 0.026758280915341208\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/15: 100%|██████████| 475/475 [01:25<00:00,  5.57it/s, loss=0.0195]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/15, Average Loss: 0.01954876429156253\n","Epoch 5/15, Validation Loss: 0.012155964391446915\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6/15: 100%|██████████| 475/475 [01:24<00:00,  5.62it/s, loss=0.0185]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/15, Average Loss: 0.018505192869587947\n","Epoch 6/15, Validation Loss: 0.016320316731428898\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7/15: 100%|██████████| 475/475 [01:24<00:00,  5.63it/s, loss=0.0182]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/15, Average Loss: 0.01819820866773003\n","Epoch 7/15, Validation Loss: 0.01559519542365515\n","Early stopping due to no improvement in training loss.\n"]}],"source":["num_epochs = 15\n","patience = 2\n","best_loss = float('inf')\n","epochs_without_improvement = 0\n","\n","for epoch in range(num_epochs):\n","    epoch_loss = 0.0\n","    progress_bar = tqdm(train_data_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", position=0, leave=True)\n","\n","    for anchor, positive, negative in progress_bar:\n","        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n","        optimizer.zero_grad()\n","        anchor_embed, positive_embed, negative_embed = model(anchor, positive, negative)\n","        loss = criterion(anchor_embed, positive_embed, negative_embed)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        progress_bar.set_postfix({\"loss\": epoch_loss / (progress_bar.n + 1)})\n","\n","    avg_epoch_loss = epoch_loss / len(train_data_loader)\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_epoch_loss}\")\n","\n","    # Validation\n","    model.eval()\n","    val_losses = []\n","\n","    with torch.no_grad():\n","        for anchor, positive, negative in val_data_loader:\n","            anchor_embed, positive_embed, negative_embed = model(anchor.to(device), positive.to(device), negative.to(device))\n","            loss_val = criterion(anchor_embed, positive_embed, negative_embed)\n","            val_losses.append(loss_val.item())\n","\n","    val_loss = sum(val_losses) / len(val_losses)\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss}\")\n","\n","    if val_loss  < best_loss:\n","        best_loss = val_loss\n","        epochs_without_improvement = 0\n","        torch.save(model.state_dict(), 'fine_tuned_model_new.pth')\n","    else:\n","        epochs_without_improvement += 1\n","\n","    if epochs_without_improvement >= patience:\n","        print(\"Early stopping due to no improvement in training loss.\")\n","        break\n","\n","    model.train()  # Set model back to training mode for next epoch"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
